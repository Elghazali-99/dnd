{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from os import listdir\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PERSONS = ['I', 'ME', 'MY', 'MINE', 'YOU', 'YOUR', 'YOURS', 'HE', 'SHE', 'HIS', 'HER', 'HIM', 'THEY', 'THEM', 'THEMSELVES', 'OUR', 'WE']\n",
    "LOCATIONS = ['LOC', 'GPE', 'ORG', 'FAC']\n",
    "ALL_ENTITIES = LOCATIONS + ['PERSON', 'HEALTH', 'MONEY', 'DATE', 'TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non Disclosure Experiment Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for filename in sorted(listdir('data/non_disclosure')):\n",
    "#     if not filename.endswith('.txt'):\n",
    "#         continue\n",
    "        \n",
    "#     with open('data/non_disclosure/' + filename) as f:\n",
    "#         content = f.read()\n",
    "#         doc = nlp(u''+content)\n",
    "#         # print(content)\n",
    "#         displacy.render(doc, style='ent', jupyter=True) \n",
    "#         # print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclosure Experiment Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for filename in sorted(listdir('data/disclosure')):\n",
    "#     if not filename.endswith('.txt'):\n",
    "#         continue\n",
    "        \n",
    "#     with open('data/disclosure/' + filename) as f:\n",
    "#         content = f.read()\n",
    "#         doc = nlp(u''+content)\n",
    "#         # print(content)\n",
    "#         displacy.render(doc, style='ent', jupyter=True) \n",
    "#         # print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Manual Labeling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from string import punctuation\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_docs(dnd, data):\n",
    "    with open(dnd + '.tsv', 'a') as f:\n",
    "        f.write(data)\n",
    "        print('Writing file done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_docs(path):\n",
    "    with open(path, 'r') as f:\n",
    "        non_disclosure = []\n",
    "        disclosure = []\n",
    "\n",
    "        text = f.read()\n",
    "        \n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        lines = sent_detector.tokenize(text.strip())\n",
    "        \n",
    "        for line in lines:\n",
    "            # remove punctuation from each token. Except ', char\n",
    "            tokens = line.split()\n",
    "            print(tokens)\n",
    "            table = str.maketrans('', '', '!\"#$%&()*+-./:;<=>?@[\\]^_`{|}~')\n",
    "            tokens = [w.translate(table) for w in tokens]\n",
    "            doc = ' '.join(tokens)\n",
    "\n",
    "            doc = nlp(u''+doc)\n",
    "            new_doc = ''\n",
    "\n",
    "            entity = 0\n",
    "            spacy_tokens = []\n",
    "            \n",
    "            for t in doc:\n",
    "                spacy_tokens.append(t.text)\n",
    "                \n",
    "                if t.ent_type_ == '':\n",
    "                    if t.text.upper() in PERSONS:\n",
    "                        entity = entity+1\n",
    "                        \n",
    "                        index = spacy_tokens.index(t.text)\n",
    "                        ## Replace with Entity Label\n",
    "                        # spacy_tokens[index] = 'PERSON'\n",
    "                    else:\n",
    "                        pass\n",
    "                elif t.ent_type_ in ALL_ENTITIES:\n",
    "                    entity = entity+1\n",
    "                    \n",
    "                    index = spacy_tokens.index(t.text)\n",
    "                    ## Replace with Entity Label\n",
    "                    # spacy_tokens[index] = t.ent_type_.upper()                    \n",
    "            \n",
    "            new_doc = ' '.join(spacy_tokens)\n",
    "            \n",
    "            if entity > 1:\n",
    "                if len(set([t.upper() for t in spacy_tokens]).intersection(set(PERSONS))) > 0:\n",
    "                    # print('DISCLOSURE: ', new_doc)\n",
    "                    # disclosure.append(new_doc)\n",
    "                    write_docs('Manually_Labeled_Data/disclosure', '1\\t' + new_doc + '\\n')\n",
    "                    continue\n",
    "                else:\n",
    "                    # print('NON-DISCLOSURE: ', new_doc)\n",
    "                    # non_disclosure.append(new_doc)\n",
    "                    write_docs('Manually_Labeled_Data/non-disclosure', '0\\t' + new_doc + '\\n')\n",
    "                    continue            \n",
    "            else:\n",
    "                # print('NON-DISCLOSURE: ', new_doc)\n",
    "                # non_disclosure.append(new_doc)\n",
    "                write_docs('Manually_Labeled_Data/non-disclosure', '0\\t' + new_doc + '\\n')\n",
    "                continue     \n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/medical_posts'\n",
    "\n",
    "# Make sure have new blank files\n",
    "# with open('Manually_Labeled_Data/non-disclosure.tsv', 'w') as f:\n",
    "#     f.write('')\n",
    "# with open('Manually_Labeled_Data/disclosure.tsv', 'w') as f:\n",
    "#     f.write('')  \n",
    "\n",
    "i = 0\n",
    "for filename in sorted(listdir(directory)):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "        \n",
    "#     if i >= 2:\n",
    "#         break      \n",
    "        \n",
    "    print(filename)\n",
    "    path = directory + '/' + filename    \n",
    "    process_docs(path)\n",
    "    i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# with open('Temp/medical.txt', 'r') as f:\n",
    "    \n",
    "#     sentences = []\n",
    "#     needed_sentences = []\n",
    "    \n",
    "#     non_disclosure = []\n",
    "#     disclosure = []\n",
    "    \n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         splitted_line = line.split('   ')\n",
    "#         if len(splitted_line) >= 3:\n",
    "#             sentence = splitted_line[2]\n",
    "            \n",
    "#             tokens = sentence.split()\n",
    "#             # remove punctuation from each token\n",
    "#             table = str.maketrans('', '', punctuation)\n",
    "#             tokens = [w.translate(table) for w in tokens]\n",
    "#             doc = ' '.join(tokens)\n",
    "\n",
    "#             doc = nlp(u''+doc)\n",
    "#             new_doc = ''\n",
    "            \n",
    "#             entity = 0            \n",
    "#             for t in doc:\n",
    "#                 if t.ent_type_ == '':\n",
    "#                     if t.text.upper() in PERSONS:\n",
    "#                         entity = entity+1\n",
    "#                         new_doc = doc.text.replace(' '+t.text+' ', ' PERSON ')\n",
    "#                         doc = nlp(u''+new_doc)\n",
    "#                     else:\n",
    "#                         pass\n",
    "#                 elif t.ent_type_ in ALL_ENTITIES:\n",
    "#                     entity = entity+1\n",
    "#                     new_doc = doc.text.replace(t.text, t.ent_type_.upper())\n",
    "#                     doc = nlp(u''+new_doc)\n",
    "            \n",
    "#             if entity != 0:\n",
    "#                 sentence = new_doc.strip() + ' #OFENTFOUND ' + str(entity)\n",
    "#                 number_of_ents = int(sentence.split('#OFENTFOUND')[1])\n",
    "#                 if number_of_ents > 1:\n",
    "#                     tokens = sentence.split('#OFENTFOUND')[0].split()\n",
    "#                     if 'PERSON' in tokens:\n",
    "#                         print('diclosure')\n",
    "#                         disclosure.append(new_doc.strip() + ' #OFENTFOUND ' + str(entity))\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         print('non-diclosure')\n",
    "#                         non_disclosure.append(new_doc.strip() + ' #OFENTFOUND ' + str(entity))\n",
    "#                         continue            \n",
    "#                 else:\n",
    "#                     print('non-diclosure')\n",
    "#                     non_disclosure.append(new_doc.strip() + ' #OFENTFOUND ' + str(entity))\n",
    "#                     continue                \n",
    "#             else:\n",
    "#                 print('non-diclosure')\n",
    "#                 non_disclosure.append(new_doc.strip() + ' #OFENTFOUND ' + str(entity))\n",
    "                \n",
    "#             sentences.append(sentence)\n",
    "#         else:\n",
    "#             pass\n",
    "        \n",
    "# print('Disclosure Sentences:', len(disclosure))\n",
    "# print('Non disclosure Sentences:', len(non_disclosure), '\\n')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding a verb with a subject otherwise expand the context**\n",
    "To Do: R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Because the syntactic relations form a tree, every word has exactly one head.\n",
    "# from spacy.symbols import nsubj, VERB\n",
    "\n",
    "# first_sentence = \"How is your son?\"\n",
    "# second_sentence = \"Got the flu.\"\n",
    "# context = \"\"\n",
    "\n",
    "# doc = nlp(u''+second_sentence)\n",
    "\n",
    "# displacy.render(doc, style='dep', jupyter=True, options={'distance' : 200})\n",
    "\n",
    "# verbs = set()\n",
    "# for possible_subject in doc:\n",
    "#     if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "#         verbs.add(possible_subject.head)\n",
    "        \n",
    "# if len(verbs) == 0:\n",
    "#     context = first_sentence + \" \" + second_sentence\n",
    "#     print(context)       \n",
    "\n",
    "# doc = nlp(u''+context.lower())\n",
    "# displacy.render(doc, style='dep', jupyter=True, options={'distance' : 200})\n",
    "\n",
    "# context = []\n",
    "# for token in doc:\n",
    "#     if token.dep_ == 'nsubj' or token.dep_ == 'dobj' or token.dep_ == 'ROOT':\n",
    "#         context.append(str(token))\n",
    "    \n",
    "# print(' '.join(context))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating numbered files from texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "with open('Manually_Labeled_Data/disclosure.tsv', encoding='utf-8') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "    \n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for row in readCSV:\n",
    "        review = row[1]\n",
    "        sentences.append(review)\n",
    "        i += 1         \n",
    "        \n",
    "# print(sentences, len(sentences))\n",
    "sentences = random.sample(sentences, len(sentences))\n",
    "# print(sentences)\n",
    "\n",
    "j = 0\n",
    "for sentence in sentences:\n",
    "    if j >= 5000:\n",
    "        break\n",
    "    print(sentence)\n",
    "    with open(\"data/d/\"+str(j).zfill(5)+\"_line.txt\", \"w\") as f:\n",
    "        f.write(sentence.strip())\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function to identify voice of sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying voice of sentence\n",
    "# Helper function\n",
    "\n",
    "# def voice_of_sentence(doc):\n",
    "#     sentence_type = \"\"\n",
    "#     for token in doc:\n",
    "#         if token.dep_ == 'nsubj':\n",
    "#             sentence_type = 'active'\n",
    "#             break\n",
    "#         elif token.dep_ == 'nsubjpass':\n",
    "#             sentence_type = 'passive'\n",
    "\n",
    "#     return sentence_type        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
