{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nuhil/miniconda3/envs/nlp/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistics Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency Parse Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He lost \\$100K in the share market in last couple of months\n",
      "He\n",
      "\\$100K\n",
      "the share market\n",
      "last couple\n",
      "months\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1350\" height=\"337.0\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">lost</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">\\$100</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">K</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">share</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">market</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">last</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">couple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">months</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,152.0 135.0,152.0 135.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L62,192.0 78,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M270,202.0 C270,152.0 335.0,152.0 335.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,204.0 L262,192.0 278,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M170,202.0 C170,102.0 340.0,102.0 340.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340.0,204.0 L348.0,192.0 332.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M370,202.0 C370,152.0 435.0,152.0 435.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M435.0,204.0 L443.0,192.0 427.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M570,202.0 C570,102.0 740.0,102.0 740.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,204.0 L562,192.0 578,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M670,202.0 C670,152.0 735.0,152.0 735.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M670,204.0 L662,192.0 678,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M470,202.0 C470,52.0 745.0,52.0 745.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,204.0 L753.0,192.0 737.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M170,202.0 C170,2.0 850.0,2.0 850.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M850.0,204.0 L858.0,192.0 842.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M970,202.0 C970,152.0 1035.0,152.0 1035.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,204.0 L962,192.0 978,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M870,202.0 C870,102.0 1040.0,102.0 1040.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1040.0,204.0 L1048.0,192.0 1032.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1070,202.0 C1070,152.0 1135.0,152.0 1135.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1135.0,204.0 L1143.0,192.0 1127.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-11\" stroke-width=\"2px\" d=\"M1170,202.0 C1170,152.0 1235.0,152.0 1235.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1235.0,204.0 L1243.0,192.0 1227.0,192.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "lost\n",
      "\\$100\n",
      "K\n",
      "in\n",
      "the\n",
      "share\n",
      "market\n",
      "in\n",
      "last\n",
      "couple\n",
      "of\n",
      "months\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'He lost \\$100K in the share market in last couple of months')\n",
    "print(doc.text)\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance' : 100, 'compact' : False})\n",
    "for t in doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named entity recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">He lost \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \\$100\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "K in the share market in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    last couple of months\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency Children**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He nsubj lost VERB []\n",
      "lost ROOT lost VERB [He, K, in]\n",
      "\\$100 compound K PROPN []\n",
      "K dobj lost VERB [\\$100, in]\n",
      "in prep K PROPN [market]\n",
      "the det market NOUN []\n",
      "share compound market NOUN []\n",
      "market pobj in ADP [the, share]\n",
      "in prep lost VERB [couple]\n",
      "last amod couple NOUN []\n",
      "couple pobj in ADP [last, of]\n",
      "of prep couple NOUN [months]\n",
      "months pobj of ADP []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Entity Recogniser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I got flu.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'I got flu.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Annotation of Custom Entity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "\n",
    "# text = 'My son is in depression because of his grades'\n",
    "# look = 'depression'\n",
    "# if text.find(look) != -1:    \n",
    "#     start = text.find(look)\n",
    "#     end = text.find(look)+len(look)\n",
    "\n",
    "# print(start, end, text[start:end])\n",
    "\n",
    "# I have the flu\n",
    "# I got a fever\n",
    "# He was identified as diabetic positive after the blood test\n",
    "# My daughter is having severe toothache \n",
    "# Heart disease is what I was afraid of and eventually got that\n",
    "# My son is in depression because of his grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Save the Custom Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nuhil/miniconda3/envs/nlp/lib/python3.6/site-packages/numpy/linalg/linalg.py:2176: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ret = sqrt(sqnorm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 42.270778634674109}\n",
      "{'ner': 35.745225217444521}\n",
      "{'ner': 30.057589331588179}\n",
      "{'ner': 24.21131648940182}\n",
      "{'ner': 26.694831935074767}\n",
      "{'ner': 21.809140086647652}\n",
      "{'ner': 28.685883187580792}\n",
      "{'ner': 24.399925323408343}\n",
      "{'ner': 20.425318012377694}\n",
      "{'ner': 23.608125195151555}\n",
      "{'ner': 22.002870753829114}\n",
      "{'ner': 22.756646265962566}\n",
      "{'ner': 17.117172025916926}\n",
      "{'ner': 17.920637648543963}\n",
      "{'ner': 22.060565742804712}\n",
      "{'ner': 15.519913633222156}\n",
      "{'ner': 10.782497671418788}\n",
      "{'ner': 20.20362781767281}\n",
      "{'ner': 16.393087865454085}\n",
      "{'ner': 13.660432046351721}\n",
      "Entities in 'He has the flu. Andrew got flu. I lost $10 bucks.'\n",
      "HEALTH the flu\n",
      "PERSON Andrew\n",
      "MONEY $10\n",
      "Saved model to model\n",
      "Loading from model\n",
      "HEALTH the flu\n",
      "PERSON Andrew\n",
      "MONEY $10\n"
     ]
    }
   ],
   "source": [
    "# new entity label\n",
    "LABEL = 'HEALTH'\n",
    "\n",
    "# training data\n",
    "# Note: If you're using an existing model, make sure to mix in examples of\n",
    "# other entity types that spaCy correctly recognized before. Otherwise, your\n",
    "# model might learn the new type, but \"forget\" what it previously knew.\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "TRAIN_DATA = [    \n",
    "    (\"I got flu.\", {\n",
    "        'entities': [(6, 9, 'HEALTH')]\n",
    "    }),\n",
    "    (\"He has the flu.\", {\n",
    "        'entities': [(7, 14, 'HEALTH')]\n",
    "    }),    \n",
    "    (\"He is sick.\", {\n",
    "        'entities': [(6, 10, 'HEALTH')]\n",
    "    }),    \n",
    "    (\"I got a fever\", {\n",
    "        'entities': [(8, 13, 'HEALTH'), (6, 13, 'HEALTH')]\n",
    "    }),        \n",
    "    (\"He was identified as diabetic positive after the blood test\", {\n",
    "        'entities': [(21, 29, 'HEALTH'), (49, 59, 'HEALTH'), (45, 59, 'HEALTH')]\n",
    "    }),            \n",
    "    (\"My daughter is having severe toothache\", {\n",
    "        'entities': [(29, 38, 'HEALTH')]\n",
    "    }),   \n",
    "    (\"Heart disease is what I was afraid of and eventually got that\", {\n",
    "        'entities': [(0, 13, 'HEALTH'), (6, 13, 'HEALTH')]\n",
    "    }),       \n",
    "    (\"My son is in depression because of his grades\", {\n",
    "        'entities': [(13, 23, 'HEALTH')]\n",
    "    }),    \n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),    \n",
    "]\n",
    "\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "#     new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "#     n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def add_new_dre(model=None, new_model_name='custom_model', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.load('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = 'He has the flu. Andrew got flu. I lost $10 bucks.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "add_new_dre('en', 'custom_model', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the Newly Trained Entity Recogniser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the flu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HEALTH</span>\n",
       "</mark>\n",
       ". I have $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    1000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " loan. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Shaka Khan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    sick\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HEALTH</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('model')\n",
    "doc = nlp(u'I have the flu. I have $1000 loan. Shaka Khan is sick.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I got \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    flu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">HEALTH</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'I got flu.')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying voice of sentence\n",
    "# Helper function\n",
    "\n",
    "# def voice_of_sentence(doc):\n",
    "#     sentence_type = \"\"\n",
    "#     for token in doc:\n",
    "#         if token.dep_ == 'nsubj':\n",
    "#             sentence_type = 'active'\n",
    "#             break\n",
    "#         elif token.dep_ == 'nsubjpass':\n",
    "#             sentence_type = 'passive'\n",
    "\n",
    "#     return sentence_type        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DRE with Noun-Phrase Relation Identification**\n",
    "Here, we extract DREs and then check the\n",
    "dependency tree to find the *noun phrase* or possible *subject* it is referring to. By other word to identify `<Subjec, Predicate, Object>` Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dre_rel(model, dre_type, texts):\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        \n",
    "#         displacy.render(doc, style='dep', jupyter=True, options={'distance' : 100, 'compact' : True})\n",
    "        displacy.render(doc, style='ent', jupyter=True)\n",
    "        \n",
    "#         for token in doc:\n",
    "#             print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "#                   [child for child in token.children])          \n",
    "        \n",
    "        relations = extract_dre_relations(doc, dre_type)\n",
    "#         print(relations)\n",
    "        for r1, r2, r3 in relations:\n",
    "            \n",
    "            entity_type = check_ent_in_noun_phrase(r3, dre_type)\n",
    "            entity_type = 'LOCATION' if entity_type in ['LOC', 'GPE', 'ORG', 'FAC'] else entity_type\n",
    "            print('Possible Subject: {}\\t Noun Phrase: {:<10}\\t Entity Type: {}\\t Entity: {}'.format(str(r1), r2.text, entity_type, r3.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying possible subject**   \n",
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PERSONS = ['I', 'ME', 'MY', 'MINE', 'YOU', 'YOUR', 'YOURS', 'HE', 'SHE', \n",
    "           'HIS', 'HER', 'HIM', 'THEY', 'THEM', 'THEMSELVES', 'OUR', 'WE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_possible_subject(root):\n",
    "    \n",
    "    subject = [w for w in root.lefts if w.dep_ == 'nsubj']\n",
    "    \n",
    "    \n",
    "    if subject:\n",
    "        subject = subject[0]\n",
    "    else:\n",
    "        subject = [w for w in root.lefts if w.dep_ == 'nsubjpass']\n",
    "        if subject:\n",
    "            subject = subject[0]\n",
    "        else:    \n",
    "            possible_subjects = []\n",
    "            for possible_subject in root.children:\n",
    "                if possible_subject.dep_ == 'dobj':\n",
    "                    possible_subjects.append(possible_subject)\n",
    "                else:\n",
    "                    possible_subjects.append('')\n",
    "            subject = possible_subjects[0]      \n",
    "     \n",
    "    if type(subject) != str and subject.ent_type_ == 'PERSON':\n",
    "        return subject\n",
    "    else:        \n",
    "        # Check if the subject is a Human\n",
    "        for person in PERSONS:\n",
    "            if person in str(subject).upper():        \n",
    "                return subject   \n",
    "        \n",
    "    return 'Not a First/Second Person'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split noun phrase again into ents\n",
    "def check_ent_in_noun_phrase(w, dre_type=None):\n",
    "    if type(w) != str and w.ent_type_ != '':\n",
    "        return w.ent_type_ if w.ent_type_ in dre_type else ''\n",
    "    else:\n",
    "        doc = nlp(str(w))\n",
    "        for token in doc:\n",
    "            if token.ent_type_ != '':\n",
    "                return token.ent_type_ if token.ent_type_ in dre_type else ''\n",
    "    return ''        \n",
    "\n",
    "def extract_dre_relations(doc, dre_type):\n",
    "    # merge entities and noun chunks into one token\n",
    "#     print(\"Ents + Noun Chunks\")\n",
    "#     print(list(doc.ents))\n",
    "#     print(list(doc.noun_chunks))\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    \n",
    "    for span in spans:\n",
    "        ms = span.merge()\n",
    "    \n",
    "#     test = filter(lambda w: check_ent_in_noun_phrase(w, dre_type), doc)\n",
    "#     print('list...', list(test))\n",
    "    \n",
    "    relations = []\n",
    "    for dre in filter(lambda w: check_ent_in_noun_phrase(w, dre_type) != '', doc):\n",
    "#         print('-', dre.text, dre.dep_)    \n",
    "        if dre.dep_ in ('attr', 'dobj'):\n",
    "            root = dre.head\n",
    "            subject = get_possible_subject(root)\n",
    "            if type(subject) == str and subject == 'Not a First/Second Person':\n",
    "                root = root\n",
    "            else:\n",
    "                root = subject\n",
    "            relations.append((subject, root, dre)) \n",
    "        elif dre.dep_ == 'pobj' and dre.head.dep_ == 'prep':\n",
    "            if dre.head.head.dep_ == 'pobj' and dre.head.head.head.dep_ == 'prep':\n",
    "                root = dre.head.head.head.head\n",
    "                if root.dep_ == 'acomp':\n",
    "                    root = root.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre))                  \n",
    "            elif dre.head.head.dep_ in ('attr', 'ROOT'):\n",
    "                root = dre.head.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre)) \n",
    "            elif dre.head.head.head.dep_ in ('attr', 'ROOT'):\n",
    "                root = dre.head.head.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre))                          \n",
    "        elif dre.dep_ == 'appos' and dre.head.dep_ == 'pobj':\n",
    "            if dre.head.head.head.dep_ == 'ROOT':\n",
    "                root = dre.head.head.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre))\n",
    "        elif dre.dep_ == 'npadvmod':\n",
    "            if dre.head.head.head.dep_ == 'ROOT':\n",
    "                root = dre.head.head.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre))\n",
    "        elif dre.dep_ == 'amod':\n",
    "            if dre.head.head.head.dep_ == 'ROOT':\n",
    "                root = dre.head.head.head\n",
    "                subject = get_possible_subject(root)                 \n",
    "                relations.append((subject, root, dre))                \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text to be tested below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "#     'I live in W Hale Street',\n",
    "#     'Meet me in the Coffee Shop',\n",
    "#     'My office is located at Main St. Boise, Idaho',\n",
    "#     'I will go to Town Square mall',\n",
    "#     'Town Square mall is a good place to hangout'\n",
    "#     'Steve works for Google',\n",
    "#     'Paris is the capital of France',\n",
    "    \n",
    "#     'Our meeting will be at 3pm in the US Bank building',\n",
    "#     'I went to office yesterday',\n",
    "#     'I am going to fire him if he does not reply within tomorrow',\n",
    "#     'We are planing to leave for Paris on 31st December in early morning',\n",
    "\n",
    "#     'My company lost $1 million dollar revenue in last quarter',\n",
    "#     'Steve owes 500 dollars to me',\n",
    "#     'I am happy with my salary of $24,500 per year',\n",
    "#     'Net income was $9.4 million compared to the prior year of $2.7 million',\n",
    "    \n",
    "#     'I have the flu',\n",
    "#     'I got fever',\n",
    "#     'My son was identified as diabetic positive',\n",
    "#     'My daughter is having severe toothache',\n",
    "#     'My son is in depression because of his grades',\n",
    "#     'Heart disease is what I was afraid of and eventually got that',\n",
    "\n",
    "#     'Worldwide production of apples in 2013 was 90.8 million tonnes',\n",
    "#     'I was not called at Starbucks by him',\n",
    "#     'My son\\'s name is Andrew and got the flu',\n",
    "    'Okey, I will meet you in Starbucks at 7pm sharp'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Linguistics Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Okey, I will meet you in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Starbucks\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    7\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "pm sharp</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Subject: I\t Noun Phrase: meet      \t Entity Type: LOCATION\t Entity: Starbucks\n"
     ]
    }
   ],
   "source": [
    "dre_rel('model', ['LOC', 'GPE', 'ORG', 'FAC','DATE', 'TIME', 'MONEY', 'HEALTH'], texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Linguistics Factors to be Considered for Rule based manipulation**   \n",
    "Helper constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "# OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "# ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "#               \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "# COMPOUNDS = [\"compound\"]\n",
    "# PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "# PERSONS = ['I', 'ME', 'MY', 'MINE', 'YOU', 'YOUR', 'YOURS', 'HE', 'SHE', \n",
    "#            'HIS', 'HER', 'HIM', 'THEY', 'THEM', 'THEMSELVES', 'OUR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding a verb with a subject otherwise expand the context**\n",
    "To Do: R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Because the syntactic relations form a tree, every word has exactly one head.\n",
    "# from spacy.symbols import nsubj, VERB\n",
    "\n",
    "# first_sentence = \"How is your son?\"\n",
    "# second_sentence = \"Got the flu.\"\n",
    "# context = \"\"\n",
    "\n",
    "# doc = nlp(u''+second_sentence)\n",
    "\n",
    "# displacy.render(doc, style='dep', jupyter=True, options={'distance' : 200})\n",
    "\n",
    "# verbs = set()\n",
    "# for possible_subject in doc:\n",
    "#     if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "#         verbs.add(possible_subject.head)\n",
    "        \n",
    "# if len(verbs) == 0:\n",
    "#     context = first_sentence + \" \" + second_sentence\n",
    "#     print(context)       \n",
    "\n",
    "# doc = nlp(u''+context.lower())\n",
    "# displacy.render(doc, style='dep', jupyter=True, options={'distance' : 200})\n",
    "\n",
    "# context = []\n",
    "# for token in doc:\n",
    "#     if token.dep_ == 'nsubj' or token.dep_ == 'dobj' or token.dep_ == 'ROOT':\n",
    "#         context.append(str(token))\n",
    "    \n",
    "# print(' '.join(context))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl and Store Data\n",
    "* Amazon product reviews = 1000\n",
    "* Amazon food reviews = 1000\n",
    "* Hotel reviews = 1000\n",
    "* Posts on medical forum = 1000\n",
    "* StackOverflow posts = 1000   \n",
    "**Total: 5000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open('/Users/nuhil/Downloads/Questions_2.csv', encoding='utf-8') as csvfile:\n",
    "#     readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    \n",
    "#     i = 0\n",
    "#     for row in readCSV:\n",
    "#         if i >= 1001:\n",
    "#             break\n",
    "#         review = row[6]\n",
    "#         with open(\"data/so_questions/\"+str(i).zfill(5)+\"_so_questions.txt\", \"w\") as f:\n",
    "#             f.write(review.strip())\n",
    "# #         print(i)\n",
    "#         i += 1         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify by Linguistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hand Craft the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Data for ANN Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data | Disclosure/Nondisclosure | Typical Model\n",
    "--- | ---\n",
    "I got the flu. | 1 | 1\n",
    "Flu is a dangerous disease. | 0 | 1\n",
    "I like eating. | 0 | 0\n",
    "Lets eat at 7pm in Pizza Hut. | 1 | 1\n",
    "Flu 7pm Pizza Hut. | 0 | 1\n",
    "Divorce gives you depression. | 0 | 1\n",
    "I got divorced. | 1 | 1\n",
    "Paris is capital of France. | 0 | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A CNN + LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split files at:  9\n",
      "Train Until: 0 - 9\n",
      "Test From:  10 - 19\n"
     ]
    }
   ],
   "source": [
    "number_of_data_in_each_class = 20\n",
    "\n",
    "# Remaining is the test dataset\n",
    "train_with_percentage = 50 \n",
    "\n",
    "split_file_at = int(number_of_data_in_each_class*(train_with_percentage/100))-1\n",
    "train_untill = split_file_at\n",
    "test_from = train_untill+1\n",
    "\n",
    "print(\"Split files at: \", split_file_at)\n",
    "print(\"Train Until: 0 -\", train_untill)\n",
    "print(\"Test From: \", test_from, \"-\", number_of_data_in_each_class-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOCATION = ['LOC', 'GPE', 'ORG', 'FAC']\n",
    "ALL_ENTITIES = LOCATION + ['PERSON', 'HEALTH', 'MONEY', 'DATE', 'TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_sentence(sen):\n",
    "    doc = nlp(u''+sen)\n",
    "\n",
    "    modified_tokens = []\n",
    "    modified_tokens_dep = []\n",
    "    modified_tokens_pos = []\n",
    "        \n",
    "    for t in doc:\n",
    "        if t.ent_type_ == '':\n",
    "            if t.text.upper() in PERSONS:\n",
    "                modified_tokens.append('PERSON')\n",
    "            else:\n",
    "                modified_tokens.append(t.text)\n",
    "        else:\n",
    "            modified_tokens.append(t.ent_type_)\n",
    "            \n",
    "#     print(modified_tokens, len(modified_tokens))\n",
    "    \n",
    "    for t in doc:\n",
    "        modified_tokens_dep.append(t.dep_)\n",
    "\n",
    "#     print(modified_tokens_dep, len(modified_tokens_dep))\n",
    "    \n",
    "    \n",
    "    for t in doc:\n",
    "        modified_tokens_pos.append(t.pos_)\n",
    "        \n",
    "#     print(modified_tokens_pos, len(modified_tokens_pos))    \n",
    "            \n",
    "    sen_length = len(modified_tokens)\n",
    "\n",
    "    i = 0   \n",
    "    start = 0\n",
    "    for token in modified_tokens:\n",
    "        if token.upper() in ALL_ENTITIES:\n",
    "            start = i\n",
    "            break\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    j = 0\n",
    "    end = None\n",
    "    for token in list(reversed(modified_tokens)):\n",
    "        if token.upper() in ALL_ENTITIES:\n",
    "            end = j\n",
    "            break\n",
    "\n",
    "        j = j+1\n",
    "\n",
    "    end = (sen_length - (end)) if end != None else 0    \n",
    "\n",
    "    return start, end, modified_tokens, modified_tokens_dep, modified_tokens_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Boundary and Different Representations in Between that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'will', 'meet', 'PERSON', 'in', 'GPE']\n",
      "['nsubj', 'aux', 'ROOT', 'dobj', 'prep', 'pobj']\n",
      "['PRON', 'VERB', 'VERB', 'PRON', 'ADP', 'PROPN']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Ok, I will meet you in Starbucks at 7pm sharp'\n",
    "start, end, modified_tokens, modified_tokens_dep, modified_tokens_pos = modify_sentence(sentence)\n",
    "print(modified_tokens[start: end])\n",
    "print(modified_tokens_dep[start: end])\n",
    "print(modified_tokens_pos[start: end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length:  135\n",
      "Top 100 vocabulary with count:  [('NOUN', 39), ('VERB', 36), ('ADP', 24), ('prep', 21), ('PROPN', 21), ('ROOT', 20), ('pobj', 20), ('nsubj', 18), ('ADJ', 15), ('PERSON', 15), ('MONEY', 13), ('compound', 13), ('det', 10), ('DET', 10), ('aux', 10), ('PRON', 10), ('ORG', 10), ('NUM', 8), ('nummod', 7), ('to', 7), ('DATE', 6), ('amod', 6), ('is', 6), ('the', 5), ('dobj', 5), ('ADV', 5), ('in', 5), ('npadvmod', 4), ('advcl', 4), ('GPE', 4), ('TIME', 4), ('on', 3), ('attr', 3), ('relcl', 3), ('acomp', 3), ('SYM', 3), ('not', 3), ('neg', 3), ('appos', 3), (',', 3), ('a', 3), ('punct', 3), ('PUNCT', 3), ('PART', 3), ('poss', 3), ('if', 3), ('mark', 3), ('LOC', 3), ('will', 3), ('of', 2), ('nmod', 2), ('am', 2), ('nsubjpass', 2), ('auxpass', 2), ('does', 2), ('advmod', 2), ('live', 2), ('cc', 2), ('conj', 2), ('CCONJ', 2), ('are', 2), ('at', 2), ('FAC', 2), ('for', 2), ('xcomp', 2), ('Turing', 1), ('was', 1), ('one', 1), ('persons', 1), ('who', 1), ('worked', 1), ('first', 1), ('computers', 1), ('Spending', 1), ('restaurant', 1), ('bad', 1), ('csubj', 1), ('yersterday', 1), ('quantmod', 1), ('nothing', 1), ('morning', 1), ('know', 1), ('bankruptcy', 1), ('legal', 1), ('process', 1), ('which', 1), ('happens', 1), ('when', 1), ('client', 1), ('have', 1), ('enough', 1), ('money', 1), ('parataxis', 1), ('very', 1), ('populated', 1), ('city', 1), ('Money', 1), ('flu', 1), ('CARDINAL', 1), ('pound', 1)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# load a doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    doc = load_doc(filename)\n",
    "    \n",
    "    start, end, modified_tokens, modified_tokens_dep, modified_tokens_pos = modify_sentence(doc)\n",
    "    \n",
    "    # print(modified_tokens)\n",
    "    vocab.update(modified_tokens)\n",
    "    \n",
    "    # print(modified_tokens_dep)    \n",
    "    vocab.update(modified_tokens_dep)\n",
    "    \n",
    "    # print(modified_tokens_pos)    \n",
    "    vocab.update(modified_tokens_pos)    \n",
    "    \n",
    "    # print('-'*50)\n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in sorted(listdir(directory)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # skip any posts in the train set\n",
    "        # skip any posts in the test set\n",
    "        file_number = int(filename[:5])\n",
    "        if is_trian and file_number > train_untill:\n",
    "            continue\n",
    "                            \n",
    "        if not is_trian and file_number < test_from:\n",
    "            continue               \n",
    "            \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "        \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('data/non_disclosure', vocab, True)\n",
    "process_docs('data/disclosure', vocab, True)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(\"Vocabulary length: \", len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(\"Top 100 vocabulary with count: \", vocab.most_common(100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that appeared more than twice:  135\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(\"Tokens that appeared more than twice: \", len(tokens))\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'data/vocab/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab, modify_type=None):\n",
    "    \n",
    "    # To Do: Check from Saved Vocab\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian, modify_type=None):\n",
    "    documents = list()\n",
    "    documents_dep = list()    \n",
    "    documents_pos = list()   \n",
    "    \n",
    "    # walk through all files in the folder\n",
    "    for filename in sorted(listdir(directory)):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue        \n",
    "            \n",
    "        # skip any posts in the test set\n",
    "        file_number = int(filename[:5])\n",
    "        if is_trian and file_number > train_untill:\n",
    "            continue\n",
    "                            \n",
    "        if not is_trian and file_number < test_from:\n",
    "            continue                        \n",
    "            \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        # load the doc            \n",
    "        doc = load_doc(path)\n",
    "        start, end, modified_tokens, modified_tokens_dep, modified_tokens_pos = modify_sentence(doc)\n",
    "        \n",
    "        # clean doc\n",
    "        if modify_type == 'dep':        \n",
    "            mod_sentence = ' '.join(modified_tokens_dep[start:end])\n",
    "            # print(mod_sentence)\n",
    "            documents_dep.append(mod_sentence)\n",
    "        elif modify_type == 'pos':      \n",
    "            mod_sentence = ' '.join(modified_tokens_pos[start:end])\n",
    "            # print(mod_sentence)\n",
    "            documents_pos.append(mod_sentence)            \n",
    "        else:    \n",
    "            mod_sentence = ' '.join(modified_tokens[start:end])\n",
    "            # print(mod_sentence)\n",
    "            documents.append(mod_sentence)\n",
    "            \n",
    "    if modify_type == 'dep':\n",
    "        return documents_dep\n",
    "    elif modify_type == 'pos':\n",
    "        return documents_pos        \n",
    "    else:    \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary:  135\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'data/vocab/vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(\"Total vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATE DATE', 'MONEY MONEY MONEY', 'MONEY MONEY MONEY', 'PERSON PERSON nothing morning MONEY MONEY MONEY', 'PERSON know , bankruptcy is a legal process which happens when a client does not have enough money', 'GPE', 'Money flu GPE', 'MONEY MONEY', '', 'ORG', 'PERSON live in LOC LOC LOC', 'PERSON in ORG ORG ORG', 'PERSON office is located at ORG ORG ORG , GPE', 'PERSON will go to FAC FAC', 'PERSON works for ORG', 'PERSON will keep MONEY MONEY', 'PERSON meeting will be at TIME TIME in the ORG ORG', 'PERSON went to HEALTH DATE', 'PERSON am going to fire PERSON if PERSON does not reply within DATE', 'PERSON are planing to leave for GPE on DATE DATE in TIME TIME'] 20 \n",
      "\n",
      "['det npadvmod', 'nmod nummod dobj', 'quantmod nummod nsubjpass', 'compound ROOT appos npadvmod nmod nummod appos', 'nsubj parataxis punct nsubj ROOT det amod attr nsubj relcl advmod det nsubj aux neg advcl amod dobj', 'nsubj', 'compound compound ROOT', 'nummod pobj', '', 'compound', 'nsubj ROOT prep compound compound pobj', 'dobj prep det compound pobj', 'poss nsubjpass auxpass ROOT prep compound compound pobj punct appos', 'nsubj aux ROOT prep compound compound', 'nsubj ROOT prep pobj', 'nsubj aux ROOT nummod dobj', 'poss nsubj aux ROOT prep nummod pobj prep det compound compound', 'nsubj ROOT prep pobj npadvmod', 'nsubj aux ROOT aux xcomp dobj mark nsubj aux neg advcl prep npadvmod', 'nsubj aux ROOT aux xcomp prep pobj prep amod pobj prep amod pobj'] 20 \n",
      "\n",
      "['DET NOUN', 'SYM NUM NOUN', 'SYM NUM NOUN', 'PROPN PROPN NOUN NOUN SYM NUM NOUN', 'PRON VERB PUNCT NOUN VERB DET ADJ NOUN ADJ VERB ADV DET NOUN VERB ADV VERB ADJ NOUN', 'PROPN', 'NOUN NOUN PROPN', 'NUM NOUN', '', 'NOUN', 'PRON VERB ADP PROPN PROPN PROPN', 'PRON ADP DET PROPN PROPN', 'ADJ NOUN VERB VERB ADP PROPN PROPN PROPN PUNCT PROPN', 'PRON VERB VERB ADP PROPN PROPN', 'PROPN VERB ADP PROPN', 'PRON VERB VERB NUM NOUN', 'ADJ NOUN VERB VERB ADP NUM NOUN ADP DET PROPN PROPN', 'PRON VERB ADP NOUN NOUN', 'PRON VERB VERB PART VERB PRON ADP PRON VERB ADV VERB ADP NOUN', 'PRON VERB VERB PART VERB ADP PROPN ADP ADJ PROPN ADP ADJ NOUN'] 20 \n",
      "\n",
      "[[17, 17], [4, 4, 4], [4, 4, 4], [5, 5, 47, 48, 4, 4, 4], [5, 49, 50, 33, 34, 51, 52, 53, 54, 55, 34, 56, 35, 36, 57, 58, 4], [21], [4, 59, 21], [4, 4], [], [11], [5, 60, 22, 28, 28, 28], [5, 22, 11, 11, 11], [5, 61, 33, 62, 37, 11, 11, 11, 21], [5, 29, 63, 23, 38, 38], [5, 64, 39, 11], [5, 29, 65, 4, 4], [5, 66, 29, 67, 37, 24, 24, 22, 68, 11, 11], [5, 69, 23, 70, 17], [5, 71, 72, 23, 73, 5, 74, 5, 35, 36, 75, 76, 17], [5, 77, 78, 23, 79, 39, 21, 80, 17, 17, 22, 24, 24], [12, 25], [40, 18, 20], [81, 18, 41], [7, 9, 30, 25, 40, 18, 30], [6, 82, 26, 6, 9, 12, 27, 83, 6, 84, 85, 12, 6, 15, 42, 43, 27, 20], [6], [7, 7, 9], [18, 13], [], [7], [6, 9, 10, 7, 7, 13], [20, 10, 12, 7, 13], [44, 41, 86, 9, 10, 7, 7, 13, 26, 30], [6, 15, 9, 10, 7, 7], [6, 9, 10, 13], [6, 15, 9, 18, 20], [44, 6, 15, 9, 10, 18, 13, 10, 12, 7, 7], [6, 9, 10, 13, 25], [6, 15, 9, 15, 45, 20, 87, 6, 15, 42, 43, 10, 25], [6, 15, 9, 15, 45, 10, 13, 10, 27, 13, 10, 27, 13], [12, 2], [31, 19, 2], [31, 19, 2], [3, 3, 2, 2, 31, 19, 2], [14, 1, 26, 2, 1, 12, 16, 2, 16, 1, 32, 12, 2, 1, 32, 1, 16, 2], [3], [2, 2, 3], [19, 2], [], [2], [14, 1, 8, 3, 3, 3], [14, 8, 12, 3, 3], [16, 2, 1, 1, 8, 3, 3, 3, 26, 3], [14, 1, 1, 8, 3, 3], [3, 1, 8, 3], [14, 1, 1, 19, 2], [16, 2, 1, 1, 8, 19, 2, 8, 12, 3, 3], [14, 1, 8, 2, 2], [14, 1, 1, 46, 1, 14, 8, 14, 1, 32, 1, 8, 2], [14, 1, 1, 46, 1, 8, 3, 8, 16, 3, 8, 16, 2]] 60\n"
     ]
    }
   ],
   "source": [
    "positive_docs = process_docs('data/disclosure', vocab, True)\n",
    "negative_docs = process_docs('data/non_disclosure', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    "print(train_docs, len(train_docs), '\\n')\n",
    "\n",
    "positive_docs_dep = process_docs('data/disclosure', vocab, True, 'dep')\n",
    "negative_docs_dep = process_docs('data/non_disclosure', vocab, True, 'dep')\n",
    "train_docs_dep = negative_docs_dep + positive_docs_dep\n",
    "print(train_docs_dep, len(train_docs_dep), '\\n')\n",
    "\n",
    "positive_docs_pos = process_docs('data/disclosure', vocab, True, 'pos')\n",
    "negative_docs_pos = process_docs('data/non_disclosure', vocab, True, 'pos')\n",
    "train_docs_pos = negative_docs_pos + positive_docs_pos\n",
    "print(train_docs_pos, len(train_docs_pos), '\\n')\n",
    "\n",
    "all_docs_in_dif_rep = train_docs + train_docs_dep + train_docs_pos\n",
    "# print(all_docs_in_dif_rep, len(all_docs_in_dif_rep), '\\n')\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(all_docs_in_dif_rep)\n",
    "\n",
    "# sequence encode\n",
    "# To Do: Check Different Encoding Scope\n",
    "# encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "encoded_docs = tokenizer.texts_to_sequences(all_docs_in_dif_rep)\n",
    "print(encoded_docs, len(encoded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Training Data:  20\n",
      "Total number of Training Data dep:  20\n",
      "Total number of Training Data pos:  20\n",
      "Total number of Training labels:  20\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "# get the biggest post as per its contents\n",
    "# max_length = max([len(s.split()) for s in train_docs])\n",
    "max_length = max([len(s.split()) for s in all_docs_in_dif_rep])\n",
    "\n",
    "# define training data\n",
    "# Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "Xtrain = pad_sequences(encoded_docs[:20], maxlen=max_length, padding='post')\n",
    "print(\"Total number of Training Data: \", len(Xtrain))\n",
    "\n",
    "\n",
    "Xtrain_dep = pad_sequences(encoded_docs[20:40], maxlen=max_length, padding='post')\n",
    "print(\"Total number of Training Data dep: \", len(Xtrain_dep))\n",
    "\n",
    "\n",
    "Xtrain_pos = pad_sequences(encoded_docs[40:60], maxlen=max_length, padding='post')\n",
    "print(\"Total number of Training Data pos: \", len(Xtrain_pos))\n",
    "\n",
    "# define training labels\n",
    "# put 0s for the first <split_file_at> entries and 1s for last <split_file_at> entries.\n",
    "# Because, in the <train_docs> list we have the public docs first\n",
    "# and private docs later.\n",
    "# From now on we are assuming 0 for public data, 1 for private data\n",
    "ytrain = array([0 for _ in range((train_untill+1))] + [1 for _ in range((train_untill+1))])\n",
    "print(\"Total number of Training labels: \", len(ytrain))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Test docs:  20\n",
      "Total number of Test docs dep:  20\n",
      "Total number of Test docs pos:  20\n",
      "Total number of Test labels:  20\n"
     ]
    }
   ],
   "source": [
    "# load all test posts\n",
    "positive_docs = process_docs('data/disclosure', vocab, False)\n",
    "negative_docs = process_docs('data/non_disclosure', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "print(\"Total number of Test docs: \", len(test_docs))\n",
    "\n",
    "positive_docs_dep = process_docs('data/disclosure', vocab, False, 'dep')\n",
    "negative_docs_dep = process_docs('data/non_disclosure', vocab, False, 'dep')\n",
    "test_docs_dep = negative_docs_dep + positive_docs_dep\n",
    "print(\"Total number of Test docs dep: \", len(test_docs_dep))\n",
    "\n",
    "positive_docs_pos = process_docs('data/disclosure', vocab, False, 'pos')\n",
    "negative_docs_pos = process_docs('data/non_disclosure', vocab, False, 'pos')\n",
    "test_docs_pos = negative_docs_pos + positive_docs_pos\n",
    "print(\"Total number of Test docs pos: \", len(test_docs_pos))\n",
    "\n",
    "all_test_docs_in_dif_rep = test_docs + test_docs_dep + test_docs_pos\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(all_test_docs_in_dif_rep)\n",
    "# pad sequences\n",
    "# Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "Xtest = pad_sequences(encoded_docs[:20], maxlen=max_length, padding='post')\n",
    "\n",
    "Xtest_dep = pad_sequences(encoded_docs[20:40], maxlen=max_length, padding='post')\n",
    "\n",
    "Xtest_pos = pad_sequences(encoded_docs[40:60], maxlen=max_length, padding='post')\n",
    "\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range((number_of_data_in_each_class - test_from))] + [1 for _ in range((number_of_data_in_each_class - test_from))])\n",
    "print(\"Total number of Test labels: \", len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (largest integer value):  88\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size (largest integer value): \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Using both CNN and LSTM**   \n",
    "CNN will give the knowledge of spatial features to the LSTM as sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # fit network\n",
    "# model.fit(Xtrain, ytrain, epochs=20, verbose=2)\n",
    "\n",
    "# # evaluate\n",
    "# loss, acc = model.evaluate(Xtest, ytest, verbose=1)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Using CNN Only with Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(10, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # fit network\n",
    "# model.fit(Xtrain, ytrain, epochs=20, verbose=2)\n",
    "\n",
    "# # evaluate\n",
    "# loss, acc = model.evaluate(Xtest, ytest, verbose=1)\n",
    "# print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Using Multichannel CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 0s - loss: 0.6877 - acc: 0.6000     \n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s - loss: 0.6636 - acc: 0.8500     \n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s - loss: 0.6216 - acc: 0.9000     \n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s - loss: 0.5781 - acc: 0.9000     \n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s - loss: 0.5432 - acc: 0.9500     \n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s - loss: 0.4821 - acc: 0.9500     \n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s - loss: 0.4243 - acc: 1.0000     \n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s - loss: 0.3460 - acc: 1.0000     \n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s - loss: 0.2623 - acc: 1.0000     \n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s - loss: 0.2124 - acc: 1.0000     \n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s - loss: 0.1590 - acc: 1.0000     \n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s - loss: 0.1165 - acc: 1.0000     \n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s - loss: 0.0834 - acc: 1.0000     \n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s - loss: 0.0541 - acc: 1.0000     \n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s - loss: 0.0362 - acc: 1.0000     \n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s - loss: 0.0297 - acc: 1.0000     \n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s - loss: 0.0304 - acc: 1.0000     \n",
      "Epoch 18/20\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.0355 - acc: 1.000 - 0s - loss: 0.0215 - acc: 1.0000     \n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s - loss: 0.0161 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s - loss: 0.0153 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11719fd68>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# channel 1\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "conv1 = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding1)\n",
    "drop1 = Dropout(0.5)(conv1)\n",
    "pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "flat1 = Flatten()(pool1)\n",
    "\n",
    "# channel 2\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "conv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
    "drop2 = Dropout(0.5)(conv2)\n",
    "pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "flat2 = Flatten()(pool2)\n",
    "\n",
    "# channel 3\n",
    "inputs3 = Input(shape=(max_length,))\n",
    "embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "conv3 = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding3)\n",
    "drop3 = Dropout(0.5)(conv3)\n",
    "pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "flat3 = Flatten()(pool3)\n",
    "\n",
    "# merge\n",
    "merged = concatenate([flat1, flat2, flat3])\n",
    "# interpretation\n",
    "dense1 = Dense(10, activation='relu')(merged)\n",
    "outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# summarize\n",
    "# print(model.summary())\n",
    "\n",
    "model.fit([Xtrain,Xtrain_dep,Xtrain_pos], ytrain, epochs=20, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.000002\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([Xtest,Xtest_dep,Xtest_pos], ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Multichannel LSTM Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 18)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 18)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 18)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 18, 100)       4256000     input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 18, 100)       4256000     input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 18, 100)       4256000     input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 100)           80400       embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 100)           80400       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 100)           80400       embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 300)           0           lstm_4[0][0]                     \n",
      "                                                                   lstm_5[0][0]                     \n",
      "                                                                   lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 100)           30100       concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 10)            1010        dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             11          dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 13,040,321\n",
      "Trainable params: 13,040,321\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 2s - loss: 0.7012 - acc: 0.5000     \n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s - loss: 0.6933 - acc: 0.5000     \n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s - loss: 0.6912 - acc: 0.5500     \n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s - loss: 0.6894 - acc: 0.5500     \n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s - loss: 0.6879 - acc: 0.7000     \n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s - loss: 0.6834 - acc: 0.8000     \n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s - loss: 0.6833 - acc: 0.7000     \n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s - loss: 0.6732 - acc: 0.7500     \n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s - loss: 0.6624 - acc: 0.7500     \n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s - loss: 0.6420 - acc: 0.7500     \n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s - loss: 0.6166 - acc: 0.8000     \n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s - loss: 0.5639 - acc: 0.8000     \n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s - loss: 0.4844 - acc: 0.9000     \n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s - loss: 0.3958 - acc: 0.8500     \n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s - loss: 0.2444 - acc: 0.9000     \n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s - loss: 0.1326 - acc: 0.9500     \n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s - loss: 0.0866 - acc: 0.9500     \n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s - loss: 0.0511 - acc: 0.9500     \n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s - loss: 0.0202 - acc: 1.0000     \n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s - loss: 0.0159 - acc: 1.0000     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132beb9e8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "# numpy.random.seed(7)\n",
    "\n",
    "max_length = 18\n",
    "vocab_size = 42560\n",
    "\n",
    "# channel 1\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "lstm1 = LSTM(100, dropout=0.2)(embedding1)\n",
    "\n",
    "# channel 2\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "lstm2 = LSTM(100, dropout=0.2)(embedding2)\n",
    "\n",
    "# channel 3\n",
    "inputs3 = Input(shape=(max_length,))\n",
    "embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "lstm3 = LSTM(100, dropout=0.2)(embedding3)\n",
    "\n",
    "# merge\n",
    "merged = concatenate([lstm1, lstm2, lstm3])\n",
    "\n",
    "# dropout\n",
    "# dropped = Dropout(0.2)(merged)\n",
    "\n",
    "# interpretation\n",
    "dense = Dense(100, activation='relu')(merged)\n",
    "dense = Dense(10, activation='relu')(dense)\n",
    "outputs = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# summarize\n",
    "print(model.summary())\n",
    "plot_model(model, show_shapes=True, to_file='multichannel-lstm.png')\n",
    "\n",
    "model.fit([Xtrain,Xtrain_dep,Xtrain_pos], ytrain, epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.000002\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([Xtest,Xtest_dep,Xtest_pos], ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Turn a doc into clean tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = modify_sentence(doc)\n",
    "    print(doc)\n",
    "\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    \n",
    "#     # remove punctuation from each token\n",
    "#     table = str.maketrans('', '', punctuation)\n",
    "#     tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "#     people = []\n",
    "#     for w in tokens:\n",
    "#         if w.upper() in PERSONS:\n",
    "#             people.append(w)    \n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) >= 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def process_new_doc(path):\n",
    "    \"\"\"\n",
    "    Process a single doc with post\n",
    "    \"\"\"\n",
    "    # define a new list\n",
    "    documents = list()\n",
    "    \n",
    "    # load doc\n",
    "    doc = load_doc(path)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # add to list\n",
    "    documents.append(tokens)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def predict_privacy(path, max_length, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Give a new unseen doc to predict it's privacy\n",
    "    \"\"\"\n",
    "    new_doc = process_new_doc(path)\n",
    "    # sequence encode\n",
    "    # To Do:\n",
    "    # 1. Will check the current <tokentizer> in memory\n",
    "    encoded_doc = tokenizer.texts_to_sequences(new_doc)\n",
    "    \n",
    "    # pad sequences\n",
    "    max_length = max_length\n",
    "    Xpredict = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # prediction\n",
    "    ypredict = model.predict(Xpredict, verbose=1)\n",
    "    print(\"Privacy Score: {0} \\nRounded To: {1}\".format(ypredict, round(ypredict[0,0])))\n",
    "    \n",
    "    return round(ypredict[0,0])\n",
    "\n",
    "def predict_privacy_by_text(text, max_length, tokenizer, model):\n",
    "    # define a new list\n",
    "    documents = list()\n",
    "    # clean doc\n",
    "    tokens = clean_doc(text)\n",
    "    # add to list\n",
    "    documents.append(tokens)\n",
    "    \n",
    "    # sequence encode\n",
    "    # To Do:\n",
    "    # 1. Will check the current <tokentizer> in memory\n",
    "    encoded_doc = tokenizer.texts_to_sequences(documents)\n",
    "    \n",
    "    # pad sequences\n",
    "    max_length = max_length\n",
    "    Xpredict = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # prediction\n",
    "    ypredict = model.predict(Xpredict, verbose=0)\n",
    "    print(\"Privacy Score: {0} \\nRounded To: {1}\".format(ypredict, round(ypredict[0,0])))\n",
    "    \n",
    "    return round(ypredict[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk.data\n",
    "# sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# new_doc = 'data/predict/content.txt'\n",
    "# print(\"\\nContent is Public\" if predict_privacy(new_doc, max_length, tokenizer, model) == 0.0 else \"\\n\\x1b[31mContent is Private\\x1b[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(new_doc) as f:\n",
    "#     text = f.read()\n",
    "#     sentences = sent_detector.tokenize(text.strip())\n",
    "        \n",
    "# for sentence in sentences:\n",
    "#     print(sentence+\"\\n\")\n",
    "#     print(\"Sentence is Public\" if predict_privacy_by_text(sentence, max_length, tokenizer, model) == 0.0 else \"\\x1b[31mSentence is Private\\x1b[0m\")\n",
    "#     print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
