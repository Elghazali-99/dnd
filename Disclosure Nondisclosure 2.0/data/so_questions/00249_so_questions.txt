<p>As far as I can tell, in spite of the countless millions or billions spent on OOP education, languages, and tools, OOP has not improved developer productivity or software reliability, nor has it reduced development costs. Few people use OOP in any rigorous sense (few people adhere to or understand principles such as LSP); there seems to be little uniformity or consistency to the approaches that people take to modelling problem domains.  All too often, the class is used simply for its syntactic sugar; it puts the functions for a record type into their own little namespace.</p>

<p>I've written a large amount of code for a wide variety of applications. Although there have been places where true substitutable subtyping played a valuable role in the application, these have been pretty exceptional. In general, though much lip service is given to talk of "re-use" the reality is that unless a piece of code does <em>exactly</em> what you want it to do, there's very little cost-effective "re-use".  It's extremely hard to design classes to be extensible <em>in the right way</em>, and so the cost of extension is normally so great that "re-use" simply isn't worthwhile.</p>

<p>In many regards, this doesn't surprise me.  The real world isn't "OO", and the idea implicit in OO--that we can model things with some class taxonomy--seems to me very fundamentally flawed (I can sit on a table, a tree stump, a car bonnet, someone's lap--but not one of those is-a chair).  Even if we move to more abstract domains, OO modelling is often difficult, counterintuitive, and ultimately unhelpful (consider the classic examples of circles/ellipses or squares/rectangles).</p>

<p>So what am I missing here? Where's the value of OOP, and why has all the time and money failed to make software any better?</p>